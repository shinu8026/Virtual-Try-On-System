{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Try-On System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentor(image_path, image_type):\n",
    "    # Load the user image\n",
    "    if image_type == 'user_image':\n",
    "        img = cv2.imread(image_path)\n",
    "    else:\n",
    "        img = image_path\n",
    "\n",
    "    # Preprocessing\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "    img_blur = cv2.GaussianBlur(img_gray, (5, 5), 0)  # Apply Gaussian blur\n",
    "\n",
    "    # Segmentation (example using GrabCut)\n",
    "    mask = np.zeros(img.shape[:2], np.uint8)\n",
    "    bgdModel = np.zeros((1, 65), np.float64)\n",
    "    fgdModel = np.zeros((1, 65), np.float64)\n",
    "    rect = (50, 50, 450, 450)  # Initial rectangle for GrabCut\n",
    "    cv2.grabCut(img, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)\n",
    "    mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "    img_segmented = img * mask2[:, :, np.newaxis]\n",
    "\n",
    "    # Clothing detection (using a pre-trained YOLOv5 model)\n",
    "    # model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "    # results = model(img_segmented)\n",
    "\n",
    "    cv2.imshow(\"Segmented Image\", img_segmented)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return img_segmented\n",
    "\n",
    "def try_on_clothing(user_image, clothing_image, keypoints):\n",
    "    # Read and pre-process user and clothing images\n",
    "    user_image = user_image\n",
    "    clothing_image = clothing_image[:,:,::-1]  # Convert BGR to RGB for OpenCV functions\n",
    "    mask = cv2.cvtColor(clothing_image, cv2.COLOR_BGR2GRAY)  # Extract clothing mask\n",
    "\n",
    "    # Applying scaling on clothing:\n",
    "    scale_factor = 1  \n",
    "    clothing_image = cv2.resize(clothing_image, None, fx=scale_factor, fy=scale_factor)\n",
    "\n",
    "    # Align clothing based on keypoints\n",
    "    user_points = np.array(keypoints, dtype=\"float32\")\n",
    "    clothing_points = np.array([(0, 0), (clothing_image.shape[1], 0), \n",
    "                                (clothing_image.shape[1], clothing_image.shape[0]), \n",
    "                                (0, clothing_image.shape[0])], dtype=\"float32\")\n",
    "    transformation_matrix = cv2.getPerspectiveTransform(user_points, clothing_points)\n",
    "\n",
    "    user_image = user_image.astype(np.uint8) \n",
    "    warped_clothing = cv2.warpPerspective(clothing_image, transformation_matrix*0.8, \n",
    "                                          (user_image.shape[1], user_image.shape[0]))\n",
    "\n",
    "    # mask resize to match user image size\n",
    "    mask = cv2.resize(mask, (user_image.shape[1], user_image.shape[0]))\n",
    "\n",
    "    # Perform bitwise operations\n",
    "    user_image = cv2.bitwise_and(user_image, user_image, mask=cv2.bitwise_not(mask))\n",
    "    composite_image = cv2.addWeighted(user_image,0.4, warped_clothing,0.6,0)\n",
    "\n",
    "    # Convert to uint8\n",
    "    composite_image = composite_image.astype(np.uint8)\n",
    "\n",
    "    # Return the composite image\n",
    "    return composite_image\n",
    "\n",
    "def pose_estimator(user_image_path):\n",
    "    # Initialize MediaPipe pose estimation\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.8) \n",
    "\n",
    "    # Load the user image\n",
    "    image = cv2.imread(user_image_path)\n",
    "\n",
    "    # Convert image to RGB format (MediaPipe expects RGB)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Perform pose estimation\n",
    "    results = pose.process(image)\n",
    "\n",
    "    # Extract keypoints\n",
    "    if results.pose_landmarks:\n",
    "        keypoints = results.pose_landmarks.landmark\n",
    "        # Access keypoints for further processing (e.g., using coordinates for alignment)\n",
    "\n",
    "    # Draw keypoints on the image\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # Convert back to BGR for OpenCV display\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    cv2.imshow(\"Pose Estimation\", image)\n",
    "    cv2.waitKey(0)\n",
    "    return keypoints, mp_pose\n",
    "\n",
    "def align_and_convert_color(clothing_image_path, user_image):\n",
    "    # Resize clothing image to be larger than user image\n",
    "    clothing_image = cv2.imread(clothing_image_path)\n",
    "    scale_factor = 1\n",
    "    clothing_image = cv2.resize(clothing_image, None, fx=scale_factor, fy=scale_factor)\n",
    "    # clothing_image = cv2.resize(clothing_image, None, fx=user_image.shape[0], fy=user_image.shape[1])\n",
    "\n",
    "    # Convert clothing images to grayscale\n",
    "    clothing_gray = cv2.cvtColor(clothing_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find contours in clothing image\n",
    "    contours, _ = cv2.findContours(clothing_gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Find the largest contour\n",
    "    if len(contours) > 0:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Get bounding box of largest contour\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "\n",
    "        # Crop clothing image to bounding box\n",
    "        clothing_image = clothing_image[y:y+h, x:x+w]\n",
    "\n",
    "    # Convert clothing image to same color space as user image\n",
    "    clothing_image = cv2.cvtColor(clothing_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return clothing_image\n",
    "\n",
    "user_image_path = r\"\\lady.jpeg\"\n",
    "clothing_image_path = r\"\\clothing.jpg\"\n",
    "\n",
    "#Another set of input images\n",
    "# user_image_path = r\"\\Men_User_Image.jpg\" #replace with the relative path\n",
    "# clothing_image_path = r\"\\Tshirt_Clothing.jpg\"\n",
    "\n",
    "# Getting segmented clothing and user images\n",
    "user_image = segmentor(user_image_path, image_type='user_image')\n",
    "clothing_image = align_and_convert_color(clothing_image_path, user_image)\n",
    "clothing_image = segmentor(clothing_image, image_type='clothing_image')\n",
    "\n",
    "# getting body keypoints\n",
    "body_keypoints, mp_pose = pose_estimator(user_image_path)\n",
    "\n",
    "# getting keypoints of specific body parts to align the clothing image to user image\n",
    "left_shoulder = body_keypoints[mp_pose.PoseLandmark.LEFT_SHOULDER]\n",
    "right_shoulder = body_keypoints[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "left_foot = body_keypoints[mp_pose.PoseLandmark.LEFT_FOOT_INDEX]\n",
    "right_foot = body_keypoints[mp_pose.PoseLandmark.RIGHT_FOOT_INDEX]\n",
    "\n",
    "# making the composite image\n",
    "keypoints = np.array([(0-60, 0-70), (clothing_image.shape[1]-60, 0-70), \n",
    "                                (clothing_image.shape[1]-60, clothing_image.shape[0]-70), \n",
    "                                (0-60, clothing_image.shape[0]-70)], dtype=\"float32\")\n",
    "# keypoints = [(100-50, 150-50), (350-50, 150-50), (350-50, 400-50), (100-50, 400-50)]\n",
    "# keypoints = [(right_shoulder.x*100, right_shoulder.y*100), (left_shoulder.x*100, left_shoulder.y*100),\n",
    "#              (right_foot.x*100, right_foot.y*100), (left_foot.x*100, left_foot.y*100)]\n",
    "composite_image = try_on_clothing(user_image, clothing_image, keypoints)\n",
    "\n",
    "cv2.imshow(\"Virtual Try-On\", composite_image[0])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
